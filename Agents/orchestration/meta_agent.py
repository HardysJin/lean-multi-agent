"""
Meta Agentæ¨¡å—
å®ç°MetaAgentç±»ï¼Œåè°ƒå¤šä¸ªspecialist agentsè¿›è¡Œç»¼åˆå†³ç­–

Meta Agentä½œä¸ºåè°ƒå™¨ï¼ˆOrchestratorï¼‰ï¼Œä¸æ˜¯ä¸“å®¶ï¼ˆSpecialistï¼‰ï¼š
- ç›´æ¥è°ƒç”¨ core agents (in-process)
- ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½å†³ç­–
- æ•´åˆ Memory System
- æ— éœ€ MCP åè®®ï¼ˆæœªæ¥å¦‚éœ€è¦å¯ä»¥åˆ›å»º MCP wrapperï¼‰
"""

import json
import asyncio
import logging
import os
import time
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from Agents.utils.llm_config import get_default_llm, LLMConfig
from Agents.utils.tool_registry import ToolRegistry
from Memory.state_manager import MultiTimeframeStateManager
from Memory.schemas import DecisionRecord, Timeframe

# è·å–logger
logger = logging.getLogger("Agent.meta-agent")


@dataclass
class AgentConnection:
    """Specialist agentè¿æ¥ä¿¡æ¯ï¼ˆin-processï¼‰"""
    name: str
    instance: Any  # Agent instance (ç›´æ¥å¼•ç”¨)
    tools: List[Dict[str, Any]]
    resources: List[Dict[str, Any]]
    description: str


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨è®°å½•"""
    agent_name: str
    tool_name: str
    arguments: Dict[str, Any]
    result: Any
    timestamp: datetime
    execution_time_ms: float


@dataclass
class MetaDecision:
    """Meta Agentçš„æœ€ç»ˆå†³ç­–"""
    symbol: str
    action: str  # BUY, SELL, HOLD
    conviction: int  # 1-10
    reasoning: str
    evidence: Dict[str, Any]  # æ¥è‡ªå„ä¸ªagentçš„è¯æ®
    tool_calls: List[ToolCall]
    timestamp: datetime
    
    def to_decision_record(self, timeframe: Timeframe = Timeframe.TACTICAL) -> DecisionRecord:
        """è½¬æ¢ä¸ºDecisionRecordç”¨äºå­˜å‚¨åˆ°Memory System"""
        return DecisionRecord(
            id=f"META_{self.symbol}_{self.timestamp.strftime('%Y%m%d_%H%M%S')}",
            timestamp=self.timestamp,
            timeframe=timeframe,
            symbol=self.symbol,
            action=self.action,
            quantity=0,  # éœ€è¦æ ¹æ®convictionè®¡ç®—
            price=0.0,  # éœ€è¦ä»evidenceä¸­æå–
            reasoning=self.reasoning,
            agent_name="meta_agent",
            conviction=float(self.conviction),
            metadata={
                'evidence': self.evidence,
                'tool_calls_count': len(self.tool_calls),
                'agents_consulted': list(set(tc.agent_name for tc in self.tool_calls))
            }
        )


class MetaAgent:
    """
    Meta Agent - åè°ƒå™¨/ç¼–æ’å™¨ï¼ˆOrchestratorï¼‰
    
    ç›´æ¥è°ƒç”¨ specialist agents (in-process)ï¼Œåè°ƒå·¥å…·è°ƒç”¨ï¼Œ
    ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½å†³ç­–ï¼Œé›†æˆ Memory Systemã€‚
    
    ä¸ä½¿ç”¨ MCP åè®®ï¼ˆå¦‚éœ€è¦å¯åˆ›å»º MCP wrapperï¼‰ã€‚
    """
    
    def __init__(
        self,
        llm_client=None,
        state_manager: Optional[MultiTimeframeStateManager] = None,
        enable_memory: bool = True
    ):
        """
        åˆå§‹åŒ–Meta Agent
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤ï¼‰
            state_manager: StateManager instance for memory integration
            enable_memory: æ˜¯å¦å¯ç”¨Memory Systemï¼ˆé»˜è®¤Trueï¼‰
        """
        # Memory System - é»˜è®¤å¯ç”¨
        if enable_memory and state_manager is None:
            # è‡ªåŠ¨åˆ›å»ºé»˜è®¤çš„state_manager
            self.state_manager = MultiTimeframeStateManager(
                sql_db_path="Data/sql/trading_memory.db",
                vector_db_path="Data/vector_db/chroma"
            )
            print("âœ“ Memory Systemè‡ªåŠ¨å¯ç”¨ (Data/sql/trading_memory.db)")
        else:
            self.state_manager = state_manager
        
        # è¿æ¥çš„agents
        self.agents: Dict[str, AgentConnection] = {}
        
        # LLM client
        self.llm_client = llm_client if llm_client else get_default_llm()
        
        # å·¥å…·è°ƒç”¨å†å²
        self.tool_call_history: List[ToolCall] = []
        
        # å†³ç­–å†å²
        self.decision_history: List[MetaDecision] = []
    
    def _extract_prompt_from_messages(self, messages: List[Any]) -> str:
        """
        ä»LangChainæ¶ˆæ¯åˆ—è¡¨ä¸­æå–promptæ–‡æœ¬ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
        Args:
            messages: LangChainæ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„promptå­—ç¬¦ä¸²
        """
        parts = []
        for msg in messages:
            if hasattr(msg, 'type') and hasattr(msg, 'content'):
                parts.append(f"[{msg.type}]: {msg.content}")
            else:
                parts.append(str(msg))
        return "\n".join(parts)
    
    def _extract_response_text(self, response: Any) -> str:
        """
        ä»LLMå“åº”å¯¹è±¡ä¸­æå–æ–‡æœ¬
        
        Args:
            response: LLMå“åº”å¯¹è±¡
            
        Returns:
            å“åº”æ–‡æœ¬
        """
        if hasattr(response, 'content'):
            return str(response.content)
        return str(response)
    
    async def _gather_technical_analysis(
        self,
        symbol: str,
        additional_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä¸»åŠ¨æ”¶é›†æŠ€æœ¯åˆ†ææ•°æ®ï¼ˆIn-Processæ¨¡å¼ï¼‰
        
        è°ƒç”¨TechnicalAgentçš„æ‰€æœ‰ç›¸å…³å·¥å…·è·å–æŠ€æœ¯æŒ‡æ ‡
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            additional_context: é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½åŒ…å«price_dataï¼‰
            
        Returns:
            æŠ€æœ¯åˆ†æç»“æœå­—å…¸
        """
        technical_data = {}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰TechnicalAgentè¿æ¥
        if 'technical' not in self.agents:
            logger.warning("TechnicalAgent not connected, skipping technical analysis")
            return {"error": "TechnicalAgent not available"}
        
        try:
            # 1. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆRSI, MACD, etc.ï¼‰
            try:
                indicators = await self.execute_tool(
                    agent_name='technical',
                    tool_name='calculate_indicators',
                    arguments={'symbol': symbol, 'period': '3mo'}
                )
                technical_data['indicators'] = indicators
            except Exception as e:
                logger.debug(f"Indicators calculation failed: {e}")
                technical_data['indicators'] = None
            
            # 2. è¯†åˆ«æ”¯æ’‘/é˜»åŠ›ä½
            try:
                support_resistance = await self.execute_tool(
                    agent_name='technical',
                    tool_name='find_support_resistance',
                    arguments={'symbol': symbol}
                )
                technical_data['support_resistance'] = support_resistance
            except Exception as e:
                logger.debug(f"Support/Resistance identification failed: {e}")
                technical_data['support_resistance'] = None
            
            # 3. æ£€æµ‹å›¾è¡¨å½¢æ€
            try:
                patterns = await self.execute_tool(
                    agent_name='technical',
                    tool_name='detect_patterns',
                    arguments={'symbol': symbol, 'lookback_days': 60}
                )
                technical_data['patterns'] = patterns
            except Exception as e:
                logger.debug(f"Pattern detection failed: {e}")
                technical_data['patterns'] = None
            
            # 4. ç”Ÿæˆäº¤æ˜“ä¿¡å·
            try:
                signals = await self.execute_tool(
                    agent_name='technical',
                    tool_name='generate_signals',
                    arguments={'symbol': symbol}
                )
                technical_data['signals'] = signals
            except Exception as e:
                logger.debug(f"Signal generation failed: {e}")
                technical_data['signals'] = None
            
            logger.info(f"Technical analysis gathered for {symbol}: {len([v for v in technical_data.values() if v])} indicators available")
            
        except Exception as e:
            logger.error(f"Error gathering technical analysis for {symbol}: {e}")
            technical_data['error'] = str(e)
        
        return technical_data
    
    async def _gather_news_sentiment(
        self,
        symbol: str
    ) -> Dict[str, Any]:
        """
        ä¸»åŠ¨æ”¶é›†æ–°é—»æƒ…ç»ªæ•°æ®ï¼ˆIn-Processæ¨¡å¼ï¼‰
        
        è°ƒç”¨NewsAgentè·å–æ–°é—»å’Œæƒ…ç»ªåˆ†æ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            æ–°é—»æƒ…ç»ªç»“æœå­—å…¸
        """
        news_data = {}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NewsAgentè¿æ¥
        if 'news' not in self.agents:
            logger.warning("NewsAgent not connected, skipping news sentiment")
            return {"error": "NewsAgent not available"}
        
        try:
            # 1. è·å–æœ€æ–°æ–°é—»
            try:
                news_articles = await self.execute_tool(
                    agent_name='news',
                    tool_name='fetch_news',
                    arguments={'symbol': symbol, 'limit': 10, 'days_back': 7}
                )
                news_data['articles'] = news_articles
                
                # 2. åˆ†ææ–°é—»æƒ…ç»ª
                if news_articles and isinstance(news_articles, list) and len(news_articles) > 0:
                    try:
                        sentiment_result = await self.execute_tool(
                            agent_name='news',
                            tool_name='analyze_sentiment',
                            arguments={'articles': news_articles}
                        )
                        news_data['sentiment_analysis'] = sentiment_result
                    except Exception as e:
                        logger.debug(f"Sentiment analysis failed: {e}")
                        news_data['sentiment_analysis'] = None
            except Exception as e:
                logger.debug(f"News fetching failed: {e}")
                news_data['articles'] = None
                news_data['sentiment_analysis'] = None
            
            # 3. ç”Ÿæˆæƒ…ç»ªæŠ¥å‘Š
            try:
                sentiment_report = await self.execute_tool(
                    agent_name='news',
                    tool_name='generate_sentiment_report',
                    arguments={'symbol': symbol, 'days_back': 7}
                )
                news_data['sentiment_report'] = sentiment_report
            except Exception as e:
                logger.debug(f"Sentiment report generation failed: {e}")
                news_data['sentiment_report'] = None
            
            logger.info(f"News sentiment gathered for {symbol}: {len([v for v in news_data.values() if v])} data points available")
            
        except Exception as e:
            logger.error(f"Error gathering news sentiment for {symbol}: {e}")
            news_data['error'] = str(e)
        
        return news_data
    
    async def _call_llm_direct(
        self,
        messages: List[Dict[str, Any]]
    ) -> str:
        """
        ç›´æ¥è°ƒç”¨LLMï¼ˆä¸ä½¿ç”¨tool callingï¼‰
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        if not self.llm_client:
            return "No LLM client available. Please configure LLM."
        
        # æ„å»ºLangChainæ¶ˆæ¯
        langchain_messages = [SystemMessage(content=self._build_system_prompt())]
        for msg in messages:
            if msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
        
        try:
            # æ—¥å¿—: LLMè°ƒç”¨å¼€å§‹
            logger.debug("[meta_agent_direct] LLM Call Starting")
            
            # æ—¥å¿—: Prompté¢„è§ˆ
            prompt_text = self._extract_prompt_from_messages(langchain_messages)
            prompt_preview = prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
            logger.debug(f"[meta_agent_direct] Prompt Preview: {prompt_preview}")
            
            # å¯é€‰ï¼šå®Œæ•´prompt
            if os.getenv('LOG_FULL_PROMPTS', '').lower() == 'true':
                logger.debug(f"[meta_agent_direct] Full Prompt:\n{prompt_text}")
            
            # è°ƒç”¨LLM
            start_time = time.time()
            response = self.llm_client.invoke(langchain_messages)
            elapsed_ms = (time.time() - start_time) * 1000
            
            # æå–å“åº”æ–‡æœ¬
            response_text = self._extract_response_text(response)
            
            # æ—¥å¿—: å“åº”æ‘˜è¦
            response_preview = response_text[:200] + "..." if len(response_text) > 200 else response_text
            logger.info(f"[meta_agent_direct] LLM Response received in {elapsed_ms:.0f}ms (~{len(response_text)} chars)")
            logger.debug(f"[meta_agent_direct] Response Preview: {response_preview}")
            
            # å¯é€‰ï¼šå®Œæ•´å“åº”
            if os.getenv('LOG_FULL_RESPONSES', '').lower() == 'true':
                logger.debug(f"[meta_agent_direct] Full Response:\n{response_text}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"[meta_agent_direct] LLM call failed: {e}")
            return f"LLM call failed: {str(e)}"
    
    async def connect_to_agent(
        self,
        agent_name: str,
        agent_instance: Any,
        description: str = ""
    ) -> None:
        """
        è¿æ¥åˆ°specialist agent (in-process)
        
        ç›´æ¥ä½¿ç”¨ core agentsï¼Œæ— éœ€ MCP åè®®ã€‚
        ä½¿ç”¨ ToolRegistry è‡ªåŠ¨å‘ç°å·¥å…·ã€‚
        
        Args:
            agent_name: Agentåç§°
            agent_instance: Core agentå®ä¾‹ï¼ˆå¦‚ MacroAgent, TechnicalAnalysisAgentï¼‰
            description: Agentæè¿°
        """
        # ä½¿ç”¨ ToolRegistry è‡ªåŠ¨å‘ç°å·¥å…·
        tools_dict = ToolRegistry.discover_tools(agent_instance)
        
        # ä¸º TechnicalAgent æ·»åŠ é»˜è®¤ resourcesï¼ˆå…¶ä»– agent å¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        resources_dict = []
        agent_class_name = agent_instance.__class__.__name__
        
        if agent_class_name == "TechnicalAnalysisAgent":
            resources_dict = [
                {
                    'uri': f'technical://{agent_name}/cache',
                    'name': 'Cache Status',
                    'description': 'View cached technical data',
                    'mimeType': 'application/json'
                },
                {
                    'uri': f'technical://{agent_name}/capabilities',
                    'name': 'Capabilities',
                    'description': 'Available indicators and patterns',
                    'mimeType': 'application/json'
                }
            ]
        
        # åˆ›å»ºè¿æ¥ï¼ˆç›´æ¥å¼•ç”¨agentå®ä¾‹ï¼‰
        connection = AgentConnection(
            name=agent_name,
            instance=agent_instance,  # ç›´æ¥å­˜å‚¨agentå®ä¾‹
            tools=tools_dict,
            resources=resources_dict,
            description=description or getattr(agent_instance, 'description', agent_name)
        )
        
        self.agents[agent_name] = connection
        print(f"âœ“ Connected to agent: {agent_name} ({len(tools_dict)} tools, {len(resources_dict)} resources)")
    
    def get_all_tools(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨å·¥å…·
        
        Returns:
            æ‰€æœ‰agentsçš„å·¥å…·åˆ—è¡¨ï¼Œæ¯ä¸ªå·¥å…·åŒ…å«agent_name
        """
        all_tools = []
        for agent_name, connection in self.agents.items():
            for tool in connection.tools:
                tool_with_agent = tool.copy()
                tool_with_agent['agent_name'] = agent_name
                all_tools.append(tool_with_agent)
        return all_tools
    
    async def execute_tool(
        self,
        agent_name: str,
        tool_name: str,
        arguments: Dict[str, Any]
    ) -> Any:
        """
        æ‰§è¡Œspecialist agentçš„å·¥å…·
        
        Args:
            agent_name: Agentåç§°
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
            
        Raises:
            ValueError: å¦‚æœagentæˆ–toolä¸å­˜åœ¨
        """
        if agent_name not in self.agents:
            raise ValueError(f"Agent '{agent_name}' not connected")
        
        connection = self.agents[agent_name]
        agent_instance = connection.instance  # è·å–agentå®ä¾‹
        
        # éªŒè¯å·¥å…·å­˜åœ¨
        tool_exists = any(t['name'] == tool_name for t in connection.tools)
        if not tool_exists:
            raise ValueError(f"Tool '{tool_name}' not found in agent '{agent_name}'")
        
        # æ‰§è¡Œå·¥å…·
        start_time = datetime.now()
        try:
            # ç›´æ¥è°ƒç”¨agentçš„æ–¹æ³•ï¼ˆä¸é€šè¿‡handle_tool_callï¼‰
            method = getattr(agent_instance, tool_name, None)
            if method is None:
                raise ValueError(f"Method '{tool_name}' not found in agent '{agent_name}'")
            
            # è°ƒç”¨æ–¹æ³•
            result = method(**arguments)
            
            # å¦‚æœè¿”å›çš„æ˜¯åç¨‹ï¼Œåˆ™await
            if hasattr(result, '__await__'):
                result = await result
                
            execution_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # è®°å½•å·¥å…·è°ƒç”¨
            tool_call = ToolCall(
                agent_name=agent_name,
                tool_name=tool_name,
                arguments=arguments,
                result=result,
                timestamp=start_time,
                execution_time_ms=execution_time
            )
            self.tool_call_history.append(tool_call)
            
            return result
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds() * 1000
            error_result = {"error": str(e)}
            
            # è®°å½•å¤±è´¥çš„è°ƒç”¨
            tool_call = ToolCall(
                agent_name=agent_name,
                tool_name=tool_name,
                arguments=arguments,
                result=error_result,
                timestamp=start_time,
                execution_time_ms=execution_time
            )
            self.tool_call_history.append(tool_call)
            
            raise
    
    async def read_resource(
        self,
        agent_name: str,
        resource_uri: str
    ) -> Any:
        """
        è¯»å–specialist agentçš„èµ„æº
        
        Args:
            agent_name: Agentåç§°
            resource_uri: èµ„æºURI
            
        Returns:
            èµ„æºå†…å®¹
            
        Raises:
            ValueError: å¦‚æœagentä¸å­˜åœ¨
        """
        if agent_name not in self.agents:
            raise ValueError(f"Agent '{agent_name}' not connected")
        
        connection = self.agents[agent_name]
        agent_instance = connection.instance
        
        # ç®€å•å®ç°ï¼šè¿”å›agentçŠ¶æ€ä¿¡æ¯
        # å®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®URIè¿”å›ä¸åŒèµ„æº
        if 'cache' in resource_uri:
            return {
                'uri': resource_uri,
                'agent': agent_name,
                'cache_info': getattr(agent_instance, '_cache', {})
            }
        elif 'capabilities' in resource_uri:
            return {
                'uri': resource_uri,
                'agent': agent_name,
                'tools': [t['name'] for t in connection.tools],
                'description': connection.description
            }
        else:
            return {'uri': resource_uri, 'agent': agent_name, 'data': 'Resource not found'}

    
    def _retrieve_memory_context(
        self,
        symbol: str,
        lookback_hours: int = 24
    ) -> Dict[str, Any]:
        """
        ä»Memory Systemæ£€ç´¢ä¸Šä¸‹æ–‡
        
        Args:
            symbol: äº¤æ˜“æ ‡çš„
            lookback_hours: å›æº¯æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            è®°å¿†ä¸Šä¸‹æ–‡å­—å…¸
        """
        if not self.state_manager:
            return {
                "note": "No state manager available",
                "recent_decisions": []
            }
        
        try:
            # è·å–è¿‘æœŸå†³ç­–ï¼ˆç›´æ¥ä»sql_storeï¼‰
            recent_decisions = self.state_manager.sql_store.get_recent_decisions(
                symbol=symbol,
                limit=5
            )
            
            # TODO: å®ç°ä»å‘é‡å­˜å‚¨æ£€ç´¢ç›¸ä¼¼å¸‚åœºå†³ç­–
            # similar_events = self.state_manager.get_similar_past_decisions(...)
            
            return {
                "recent_decisions": [
                    {
                        "action": d.action,
                        "confidence": d.conviction,
                        "reasoning": d.reasoning,
                        "timestamp": d.timestamp.isoformat(),
                        "timeframe": str(d.timeframe)
                    }
                    for d in recent_decisions
                ]
            }
        except Exception as e:
            return {
                "error": f"Failed to retrieve memory: {str(e)}",
                "recent_decisions": []
            }
    
    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        agents_info = "\n".join([
            f"- {name}: {conn.description} ({len(conn.tools)} tools)"
            for name, conn in self.agents.items()
        ])
        
        return f"""You are a Meta Agent coordinating multiple specialist agents for quantitative trading.

Connected Agents:
{agents_info}

Your role:
1. Analyze the current situation including macro and sector context
2. Consider current portfolio positions and available capital
3. Respect all constraints provided (especially allow_long/allow_short)
4. Decide which specialist agents to consult
5. Call appropriate tools to gather information
6. Synthesize all inputs into a final trading decision
7. Provide clear reasoning for your decision

Context Priority:
1. **Constraints** (MUST follow): Risk limits, trading restrictions from macro environment
2. **Portfolio State**: Current holdings, available cash, position sizes
3. **Macro Context**: Market regime, interest rates, overall risk level
4. **Sector Context**: Industry trends, rotation signals, relative strength
5. **Memory**: Historical decisions and patterns
6. **Technical/News**: Individual stock analysis

Portfolio Considerations:
- Check if already holding the symbol (avoid redundant buys)
- Consider position concentration (max_position_size constraint)
- Ensure sufficient cash for new positions
- Evaluate if position adjustment (scaling in/out) is needed
- Consider realized/unrealized PnL when making decisions

Constraint Enforcement:
- If allow_long=False: DO NOT recommend BUY
- If allow_short=False: DO NOT recommend short positions
- If max_position_size specified: Consider position sizing
- If max_risk_per_trade specified: Adjust conviction accordingly
- If insufficient cash: DO NOT recommend BUY

Trading Actions:
- BUY: Strong evidence to enter long position (only if constraints allow AND sufficient cash)
- SELL: Strong evidence to exit or enter short position  
- HOLD: Insufficient evidence, conflicting signals, constraints prohibit action, or already at target position

Conviction Score (1-10):
- 1-3: Low conviction, weak signals
- 4-6: Moderate conviction, some supporting evidence
- 7-8: High conviction, strong evidence from multiple sources
- 9-10: Very high conviction, overwhelming evidence

Always consider:
- Multiple timeframes and perspectives
- Risk management principles (from constraints)
- Current portfolio exposure and diversification
- Available capital and position sizing
- Macro environment alignment
- Sector trends and rotation
- Historical context from memory
- Confluence of signals from different agents"""
    
    def _format_tools_for_llm(self) -> List[Dict[str, Any]]:
        """
        å°†å·¥å…·æ ¼å¼åŒ–ä¸ºAnthropic tool callingæ ¼å¼
        
        Returns:
            Anthropicå·¥å…·å®šä¹‰åˆ—è¡¨
        """
        tools = []
        for agent_name, connection in self.agents.items():
            for tool in connection.tools:
                # Anthropic toolæ ¼å¼
                anthropic_tool = {
                    "name": f"{agent_name}__{tool['name']}",  # åŠ ä¸Šagentå‰ç¼€é¿å…å†²çª
                    "description": f"[{agent_name}] {tool['description']}",
                    "input_schema": tool['inputSchema']
                }
                tools.append(anthropic_tool)
        return tools
    
    async def _call_llm_with_tools(
        self,
        messages: List[Dict[str, Any]],
        max_iterations: int = 5
    ) -> Tuple[str, List[ToolCall]]:
        """
        è°ƒç”¨LLMï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ (ä½¿ç”¨LangChainçš„tool binding)
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
            
        Returns:
            (æœ€ç»ˆå“åº”æ–‡æœ¬, å·¥å…·è°ƒç”¨åˆ—è¡¨)
        """
        if not self.llm_client:
            return "No LLM client available. Please configure LLM.", []
        
        # ä½¿ç”¨ LangChain çš„ tool calling æ”¯æŒ
        from langchain_core.tools import tool
        from langchain_core.messages import AIMessage, ToolMessage
        
        tool_calls_made = []
        
        # å°†æˆ‘ä»¬çš„å·¥å…·è½¬æ¢ä¸ºLangChainå·¥å…·æ ¼å¼
        langchain_tools = self._create_langchain_tools()
        
        if not langchain_tools:
            # å¦‚æœæ²¡æœ‰å·¥å…·ï¼Œç›´æ¥è°ƒç”¨LLM
            langchain_messages = [SystemMessage(content=self._build_system_prompt())]
            for msg in messages:
                if msg["role"] == "user":
                    langchain_messages.append(HumanMessage(content=msg["content"]))
            
            try:
                # æ—¥å¿—: LLMè°ƒç”¨å¼€å§‹
                logger.debug("[meta_agent_no_tools] LLM Call Starting")
                
                # æ—¥å¿—: Prompté¢„è§ˆ
                prompt_text = self._extract_prompt_from_messages(langchain_messages)
                prompt_preview = prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
                logger.debug(f"[meta_agent_no_tools] Prompt Preview: {prompt_preview}")
                
                # å¯é€‰ï¼šå®Œæ•´promptï¼ˆéœ€è¦ç¯å¢ƒå˜é‡ï¼‰
                if os.getenv('LOG_FULL_PROMPTS', '').lower() == 'true':
                    logger.debug(f"[meta_agent_no_tools] Full Prompt:\n{prompt_text}")
                
                # è°ƒç”¨LLM
                start_time = time.time()
                response = self.llm_client.invoke(langchain_messages)
                elapsed_ms = (time.time() - start_time) * 1000
                
                # æå–å“åº”æ–‡æœ¬
                response_text = self._extract_response_text(response)
                
                # æ—¥å¿—: å“åº”æ‘˜è¦
                response_preview = response_text[:200] + "..." if len(response_text) > 200 else response_text
                logger.info(f"[meta_agent_no_tools] LLM Response received in {elapsed_ms:.0f}ms (~{len(response_text)} chars)")
                logger.debug(f"[meta_agent_no_tools] Response Preview: {response_preview}")
                
                # å¯é€‰ï¼šå®Œæ•´å“åº”
                if os.getenv('LOG_FULL_RESPONSES', '').lower() == 'true':
                    logger.debug(f"[meta_agent_no_tools] Full Response:\n{response_text}")
                
                return response.content, []
            except Exception as e:
                logger.error(f"[meta_agent_no_tools] LLM call failed: {e}")
                return f"LLM call failed: {str(e)}", []
        
        # ç»‘å®šå·¥å…·åˆ°LLM
        try:
            llm_with_tools = self.llm_client.bind_tools(langchain_tools)
        except AttributeError:
            # å¦‚æœLLMä¸æ”¯æŒbind_toolsï¼ˆå¦‚MockLLMï¼‰ï¼Œå›é€€åˆ°ç®€å•æ¨¡å¼
            langchain_messages = [SystemMessage(content=self._build_system_prompt())]
            for msg in messages:
                if msg["role"] == "user":
                    langchain_messages.append(HumanMessage(content=msg["content"]))
            
            try:
                # æ—¥å¿—: LLMè°ƒç”¨å¼€å§‹
                logger.debug("[meta_agent_fallback] LLM Call Starting (no bind_tools support)")
                
                # æ—¥å¿—: Prompté¢„è§ˆ
                prompt_text = self._extract_prompt_from_messages(langchain_messages)
                prompt_preview = prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
                logger.debug(f"[meta_agent_fallback] Prompt Preview: {prompt_preview}")
                
                # å¯é€‰ï¼šå®Œæ•´prompt
                if os.getenv('LOG_FULL_PROMPTS', '').lower() == 'true':
                    logger.debug(f"[meta_agent_fallback] Full Prompt:\n{prompt_text}")
                
                # è°ƒç”¨LLM
                start_time = time.time()
                response = self.llm_client.invoke(langchain_messages)
                elapsed_ms = (time.time() - start_time) * 1000
                
                # æå–å“åº”æ–‡æœ¬
                response_text = self._extract_response_text(response)
                
                # æ—¥å¿—: å“åº”æ‘˜è¦
                response_preview = response_text[:200] + "..." if len(response_text) > 200 else response_text
                logger.info(f"[meta_agent_fallback] LLM Response received in {elapsed_ms:.0f}ms (~{len(response_text)} chars)")
                logger.debug(f"[meta_agent_fallback] Response Preview: {response_preview}")
                
                # å¯é€‰ï¼šå®Œæ•´å“åº”
                if os.getenv('LOG_FULL_RESPONSES', '').lower() == 'true':
                    logger.debug(f"[meta_agent_fallback] Full Response:\n{response_text}")
                
                return response.content, []
            except Exception as e:
                logger.error(f"[meta_agent_fallback] LLM call failed: {e}")
                return f"LLM call failed: {str(e)}", []
        
        # æ„å»ºåˆå§‹æ¶ˆæ¯
        langchain_messages = [SystemMessage(content=self._build_system_prompt())]
        for msg in messages:
            if msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
        
        # è¿­ä»£è°ƒç”¨ï¼Œæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        for iteration in range(max_iterations):
            try:
                # æ—¥å¿—: LLMè°ƒç”¨å¼€å§‹
                logger.debug(f"[meta_agent_iter_{iteration}] LLM Call Starting")
                
                # æ—¥å¿—: Prompté¢„è§ˆï¼ˆåªåœ¨ç¬¬ä¸€è½®æˆ–DEBUGçº§åˆ«æ—¶ï¼‰
                if iteration == 0 or os.getenv('LOG_FULL_PROMPTS', '').lower() == 'true':
                    prompt_text = self._extract_prompt_from_messages(langchain_messages)
                    prompt_preview = prompt_text[:200] + "..." if len(prompt_text) > 200 else prompt_text
                    logger.debug(f"[meta_agent_iter_{iteration}] Prompt Preview: {prompt_preview}")
                    
                    # å¯é€‰ï¼šå®Œæ•´prompt
                    if os.getenv('LOG_FULL_PROMPTS', '').lower() == 'true':
                        logger.debug(f"[meta_agent_iter_{iteration}] Full Prompt:\n{prompt_text}")
                
                # è°ƒç”¨LLM
                start_time = time.time()
                response = llm_with_tools.invoke(langchain_messages)
                elapsed_ms = (time.time() - start_time) * 1000
                
                # æå–å“åº”æ–‡æœ¬
                response_text = self._extract_response_text(response)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
                tool_count = len(response.tool_calls) if has_tool_calls else 0
                
                # æ—¥å¿—: å“åº”æ‘˜è¦
                logger.info(f"[meta_agent_iter_{iteration}] LLM Response received in {elapsed_ms:.0f}ms (~{len(response_text)} chars, {tool_count} tool calls)")
                
                if not has_tool_calls:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå“åº”
                    response_preview = response_text[:200] + "..." if len(response_text) > 200 else response_text
                    logger.debug(f"[meta_agent_iter_{iteration}] Final Response Preview: {response_preview}")
                    
                    # å¯é€‰ï¼šå®Œæ•´å“åº”
                    if os.getenv('LOG_FULL_RESPONSES', '').lower() == 'true':
                        logger.debug(f"[meta_agent_iter_{iteration}] Full Response:\n{response_text}")
                    
                    return response.content, tool_calls_made
                
                # æœ‰å·¥å…·è°ƒç”¨ï¼Œè®°å½•å·¥å…·åç§°
                tool_names = [tc['name'] for tc in response.tool_calls]
                logger.debug(f"[meta_agent_iter_{iteration}] Tool calls requested: {', '.join(tool_names)}")
                
                # æ·»åŠ AIå“åº”åˆ°æ¶ˆæ¯å†å²
                langchain_messages.append(response)
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                for tool_call in response.tool_calls:
                    tool_name = tool_call['name']
                    tool_args = tool_call['args']
                    tool_id = tool_call.get('id', f'call_{iteration}')
                    
                    # è§£æagent_nameå’Œactual_tool_name
                    if "__" in tool_name:
                        agent_name, actual_tool_name = tool_name.split("__", 1)
                    else:
                        # å°è¯•æŸ¥æ‰¾å·¥å…·æ‰€å±çš„agent
                        agent_name = None
                        for name, conn in self.agents.items():
                            if any(t['name'] == tool_name for t in conn.tools):
                                agent_name = name
                                actual_tool_name = tool_name
                                break
                        
                        if not agent_name:
                            # å·¥å…·æœªæ‰¾åˆ°
                            error_msg = f"Tool '{tool_name}' not found"
                            langchain_messages.append(
                                ToolMessage(
                                    content=json.dumps({"error": error_msg}),
                                    tool_call_id=tool_id
                                )
                            )
                            continue
                    
                    # æ‰§è¡Œå·¥å…·
                    try:
                        result = await self.execute_tool(
                            agent_name=agent_name,
                            tool_name=actual_tool_name,
                            arguments=tool_args
                        )
                        
                        # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                        langchain_messages.append(
                            ToolMessage(
                                content=json.dumps(result, default=str),
                                tool_call_id=tool_id
                            )
                        )
                        
                        # è®°å½•åˆ°tool_calls_made
                        tool_calls_made.append(self.tool_call_history[-1])
                        
                    except Exception as e:
                        # å·¥å…·æ‰§è¡Œå¤±è´¥
                        error_msg = f"Tool execution failed: {str(e)}"
                        langchain_messages.append(
                            ToolMessage(
                                content=json.dumps({"error": error_msg}),
                                tool_call_id=tool_id
                            )
                        )
            
            except Exception as e:
                return f"Error during LLM tool calling: {str(e)}", tool_calls_made
        
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæœ€åå†è°ƒç”¨ä¸€æ¬¡è·å–æœ€ç»ˆç­”æ¡ˆ
        try:
            final_response = llm_with_tools.invoke(langchain_messages)
            return final_response.content, tool_calls_made
        except Exception as e:
            return f"Max iterations reached. Last error: {str(e)}", tool_calls_made
    
    def _create_langchain_tools(self) -> List[Any]:
        """
        å°†MCPå·¥å…·è½¬æ¢ä¸ºLangChainå·¥å…·æ ¼å¼
        
        Returns:
            LangChainå·¥å…·åˆ—è¡¨
        """
        from langchain_core.tools import StructuredTool
        from pydantic import BaseModel, Field, create_model
        
        langchain_tools = []
        
        for agent_name, connection in self.agents.items():
            for tool in connection.tools:
                tool_name = f"{agent_name}__{tool['name']}"
                tool_description = f"[{agent_name}] {tool['description']}"
                
                # åˆ›å»ºè¾“å…¥æ¨¡å‹
                input_schema = tool['inputSchema']
                properties = input_schema.get('properties', {})
                required = input_schema.get('required', [])
                
                # æ„å»ºPydanticå­—æ®µ
                fields = {}
                for prop_name, prop_schema in properties.items():
                    field_type = str  # é»˜è®¤ç±»å‹
                    if prop_schema.get('type') == 'integer':
                        field_type = int
                    elif prop_schema.get('type') == 'number':
                        field_type = float
                    elif prop_schema.get('type') == 'boolean':
                        field_type = bool
                    
                    # è®¾ç½®é»˜è®¤å€¼
                    default = ... if prop_name in required else None
                    
                    fields[prop_name] = (
                        field_type,
                        Field(
                            default=default,
                            description=prop_schema.get('description', '')
                        )
                    )
                
                # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨ç©ºæ¨¡å‹
                if not fields:
                    fields = {'__dummy__': (str, Field(default='', description='No parameters'))}
                
                # åˆ›å»ºè¾“å…¥æ¨¡å‹
                InputModel = create_model(
                    f"{tool_name}_input",
                    **fields
                )
                
                # åˆ›å»ºå·¥å…·æ‰§è¡Œå‡½æ•°
                def make_tool_func(agent_name, tool_name):
                    async def tool_func(**kwargs):
                        # ç§»é™¤dummyå‚æ•°
                        kwargs.pop('__dummy__', None)
                        result = await self.execute_tool(
                            agent_name=agent_name,
                            tool_name=tool_name,
                            arguments=kwargs
                        )
                        return json.dumps(result, default=str)
                    return tool_func
                
                # åˆ›å»ºLangChainå·¥å…·
                lc_tool = StructuredTool(
                    name=tool_name,
                    description=tool_description,
                    func=make_tool_func(agent_name, tool['name']),
                    args_schema=InputModel,
                    coroutine=make_tool_func(agent_name, tool['name'])
                )
                
                langchain_tools.append(lc_tool)
        
        return langchain_tools
    
    async def analyze_and_decide(
        self,
        symbol: str,
        query: Optional[str] = None,
        additional_context: Optional[Dict[str, Any]] = None,
        macro_context: Optional[Dict[str, Any]] = None,
        sector_context: Optional[Dict[str, Any]] = None,
        constraints: Optional[Dict[str, Any]] = None,
        current_time: Optional[datetime] = None
    ) -> MetaDecision:
        """
        åˆ†æå¹¶åšå‡ºäº¤æ˜“å†³ç­–
        
        è¿™æ˜¯Meta Agentçš„æ ¸å¿ƒæ–¹æ³•ï¼ˆIn-Processæ¨¡å¼ï¼‰ï¼š
        1. ä»Memory Systemæ£€ç´¢ä¸Šä¸‹æ–‡
        2. æ¥æ”¶å®è§‚å’Œè¡Œä¸šèƒŒæ™¯
        3. ä¸»åŠ¨è°ƒç”¨TechnicalAgentå’ŒNewsAgentæ”¶é›†æ•°æ®
        4. å°†æ‰€æœ‰ä¿¡æ¯æ•´åˆåˆ°promptä¸­
        5. LLMåŸºäºå®Œæ•´ä¿¡æ¯åšå†³ç­–
        6. åº”ç”¨çº¦æŸæ¡ä»¶
        
        Args:
            symbol: äº¤æ˜“æ ‡çš„
            query: å¯é€‰çš„å…·ä½“é—®é¢˜ï¼ˆå¦‚"Should I buy AAPL?"ï¼‰
            additional_context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«price_dataç­‰ï¼‰
            macro_context: å®è§‚ç¯å¢ƒèƒŒæ™¯ï¼ˆæ¥è‡ªMacroAgentï¼‰
            sector_context: è¡Œä¸šåˆ†æèƒŒæ™¯ï¼ˆæ¥è‡ªSectorAgentï¼‰
            constraints: çº¦æŸæ¡ä»¶ï¼ˆé£é™©æ§åˆ¶å‚æ•°ï¼‰
            current_time: å½“å‰æ—¶é—´ï¼ˆå›æµ‹æ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡æ‹Ÿæ—¥æœŸï¼Œå®ç›˜æ¨¡å¼ä¸‹ä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¶é—´ï¼‰
            
        Returns:
            MetaDecisionå¯¹è±¡
        """
        # ä½¿ç”¨æä¾›çš„æ—¶é—´æˆ–å½“å‰æ—¶é—´
        decision_time = current_time if current_time is not None else datetime.now()
        
        # 0. æ£€æŸ¥çº¦æŸæ¡ä»¶ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if constraints:
            # æ£€æŸ¥æ˜¯å¦å…è®¸äº¤æ˜“
            if not constraints.get('allow_long', True) and not constraints.get('allow_short', False):
                # ç¦æ­¢åšå¤šåˆç¦æ­¢åšç©º = åªèƒ½HOLD
                return MetaDecision(
                    symbol=symbol,
                    action='HOLD',
                    conviction=10,
                    reasoning='Market constraints prohibit all trading (ç†Šå¸‚ç¦æ­¢åšå¤š)',
                    evidence={'constraints': constraints},
                    tool_calls=[],
                    timestamp=decision_time
                )
        
        # 1. æ£€ç´¢è®°å¿†ä¸Šä¸‹æ–‡
        memory_context = self._retrieve_memory_context(symbol)
        
        # 2. ä¸»åŠ¨æ”¶é›†æŠ€æœ¯åˆ†æå’Œæ–°é—»æ•°æ®ï¼ˆIn-Processæ¨¡å¼ï¼‰
        technical_analysis = await self._gather_technical_analysis(symbol, additional_context)
        news_sentiment = await self._gather_news_sentiment(symbol)
        
        # 3. æ„å»ºå®Œæ•´çš„å†³ç­–ä¸Šä¸‹æ–‡
        context_str = json.dumps({
            "symbol": symbol,
            "memory": memory_context,
            "market_data": additional_context or {},
            "technical_analysis": technical_analysis,
            "news_sentiment": news_sentiment,
            "macro": macro_context or {},
            "sector": sector_context or {},
            "constraints": constraints or {}
        }, indent=2, default=str)
        
        # 4. æ„å»ºå¢å¼ºçš„prompt
        user_message = f"""Analyze the trading opportunity for {symbol}.

You have access to comprehensive market intelligence:

{context_str}

{'Question: ' + query if query else ''}

Based on the above information, please:
1. Analyze the technical indicators (RSI, MACD, moving averages, support/resistance)
2. Consider the news sentiment and market momentum
3. Evaluate the macro environment and sector trends
4. Apply risk constraints
5. Synthesize all signals into a coherent trading decision

Provide a clear trading decision with the following format:
ACTION: [BUY/SELL/HOLD]
CONVICTION: [1-10]
REASONING: [detailed explanation citing specific technical indicators, news sentiment, and macro factors]"""
        
        messages = [{"role": "user", "content": user_message}]
        
        # 5. è°ƒç”¨LLMè·å–å†³ç­–ï¼ˆç›´æ¥è°ƒç”¨ï¼Œä¸éœ€è¦tool callingï¼‰
        final_response = await self._call_llm_direct(messages)
        
        # DEBUG: Log the response
        logger.info(f"ğŸ” LLM Response for {symbol}: {final_response[:200]}...")
        
        # 6. è§£æå†³ç­–
        decision = self._parse_decision(
            symbol=symbol,
            response=final_response,
            tool_calls=[],  # In-processæ¨¡å¼ä¸‹ï¼Œå·¥å…·å·²ç»ä¸»åŠ¨è°ƒç”¨
            decision_time=decision_time
        )
        
        # DEBUG: Log parsed decision
        logger.info(f"ğŸ” Parsed Decision: action={decision.action}, conviction={decision.conviction}")
        
        # 7. æ·»åŠ è¯æ®
        decision.evidence.update({
            'technical_analysis': technical_analysis,
            'news_sentiment': news_sentiment
        })
        
        # 8. å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
        if self.state_manager:
            try:
                decision_record = decision.to_decision_record()
                self.state_manager.store_decision(decision_record)
            except Exception as e:
                logger.warning(f"Failed to store decision in memory: {e}")
        
        # 9. è®°å½•åˆ°å†å²
        self.decision_history.append(decision)
        
        return decision
    
    def _parse_decision(
        self,
        symbol: str,
        response: str,
        tool_calls: List[ToolCall],
        decision_time: datetime
    ) -> MetaDecision:
        """
        ä»LLMå“åº”ä¸­è§£æå†³ç­–
        
        Args:
            symbol: äº¤æ˜“æ ‡çš„
            response: LLMå“åº”æ–‡æœ¬
            tool_calls: æ‰§è¡Œçš„å·¥å…·è°ƒç”¨
            decision_time: å†³ç­–æ—¶é—´
            
        Returns:
            MetaDecisionå¯¹è±¡
        """
        # æå–ACTION
        action = "HOLD"  # é»˜è®¤
        for line in response.split('\n'):
            if line.strip().startswith("ACTION:"):
                action_text = line.split("ACTION:", 1)[1].strip()
                if "BUY" in action_text.upper():
                    action = "BUY"
                elif "SELL" in action_text.upper():
                    action = "SELL"
                else:
                    action = "HOLD"
                break
        
        # æå–CONVICTION
        conviction = 5  # é»˜è®¤
        for line in response.split('\n'):
            if line.strip().startswith("CONVICTION:"):
                conviction_text = line.split("CONVICTION:", 1)[1].strip()
                try:
                    conviction = int(conviction_text.split()[0])
                    conviction = max(1, min(10, conviction))  # é™åˆ¶åœ¨1-10
                except (ValueError, IndexError):
                    conviction = 5
                break
        
        # æå–REASONING
        reasoning_lines = []
        in_reasoning = False
        for line in response.split('\n'):
            if line.strip().startswith("REASONING:"):
                reasoning_lines.append(line.split("REASONING:", 1)[1].strip())
                in_reasoning = True
            elif in_reasoning:
                if line.strip() and not line.strip().startswith(("ACTION:", "CONVICTION:")):
                    reasoning_lines.append(line.strip())
        
        reasoning = " ".join(reasoning_lines) if reasoning_lines else response
        
        # æ”¶é›†è¯æ®
        evidence = {
            "raw_response": response,
            "tools_used": [
                {
                    "agent": tc.agent_name,
                    "tool": tc.tool_name,
                    "result_summary": str(tc.result)[:200] + "..." if len(str(tc.result)) > 200 else str(tc.result)
                }
                for tc in tool_calls
            ]
        }
        
        return MetaDecision(
            symbol=symbol,
            action=action,
            conviction=conviction,
            reasoning=reasoning,
            evidence=evidence,
            tool_calls=tool_calls,
            timestamp=decision_time
        )
    
    def get_agent_info(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–agentä¿¡æ¯
        
        Args:
            agent_name: Agentåç§°
            
        Returns:
            Agentä¿¡æ¯å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if agent_name not in self.agents:
            return None
        
        conn = self.agents[agent_name]
        return {
            "name": conn.name,
            "description": conn.description,
            "tools": conn.tools,
            "resources": conn.resources
        }
    
    def list_agents(self) -> List[str]:
        """è·å–æ‰€æœ‰è¿æ¥çš„agentåç§°"""
        return list(self.agents.keys())
    
    def get_tool_call_history(self, limit: Optional[int] = None) -> List[ToolCall]:
        """
        è·å–å·¥å…·è°ƒç”¨å†å²
        
        Args:
            limit: è¿”å›çš„æœ€å¤§æ•°é‡
            
        Returns:
            å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        if limit:
            return self.tool_call_history[-limit:]
        return self.tool_call_history
    
    def get_decision_history(self, limit: Optional[int] = None) -> List[MetaDecision]:
        """
        è·å–å†³ç­–å†å²
        
        Args:
            limit: è¿”å›çš„æœ€å¤§æ•°é‡
            
        Returns:
            å†³ç­–åˆ—è¡¨
        """
        if limit:
            return self.decision_history[-limit:]
        return self.decision_history
    
    def clear_history(self) -> None:
        """æ¸…ç©ºå†å²è®°å½•"""
        self.tool_call_history.clear()
        self.decision_history.clear()


# ä¾¿æ·å‡½æ•°
async def create_meta_agent_with_technical(
    llm_config: Optional[LLMConfig] = None,
    state_manager: Optional[MultiTimeframeStateManager] = None,
    algorithm: Any = None
) -> MetaAgent:
    """
    åˆ›å»ºMeta Agentå¹¶è¿æ¥Technical Agent
    
    è¿™æ˜¯ä¸€ä¸ªä¾¿æ·å‡½æ•°ï¼Œç”¨äºå¿«é€Ÿè®¾ç½®åŸºç¡€é…ç½®ã€‚
    
    Args:
        llm_config: LLM configuration (uses default if None) - DEPRECATED, use llm_client
        state_manager: StateManager instance
        algorithm: LEAN algorithm instance (optional) - DEPRECATED, not used in core agents
        
    Returns:
        é…ç½®å¥½çš„MetaAgentå®ä¾‹
    """
    from Agents.core import TechnicalAnalysisAgent
    
    # åˆ›å»ºMeta Agentï¼ˆä½¿ç”¨æ–°APIï¼‰
    meta = MetaAgent(
        llm_client=llm_config.get_llm() if llm_config else None,
        state_manager=state_manager,
        enable_memory=state_manager is not None
    )
    
    # åˆ›å»ºå¹¶è¿æ¥Technical Agentï¼ˆä¸éœ€è¦algorithmå‚æ•°ï¼‰
    technical = TechnicalAnalysisAgent()
    await meta.connect_to_agent(
        agent_name="technical",
        agent_instance=technical,
        description="Technical analysis specialist providing indicators, signals, patterns, and support/resistance levels"
    )
    
    return meta


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    async def main():
        # åˆ›å»ºMeta Agent
        meta = MetaAgent(
            enable_memory=False  # ä½¿ç”¨æ–°API
        )
        
        # è¿æ¥Technical Agent
        from Agents.core import TechnicalAnalysisAgent
        technical = TechnicalAnalysisAgent()
        await meta.connect_to_agent(
            agent_name="technical",
            agent_instance=technical,
            description="Technical analysis specialist"
        )
        
        # æŸ¥çœ‹å¯ç”¨å·¥å…·
        print("Available tools:")
        for tool in meta.get_all_tools():
            print(f"  - {tool['agent_name']}.{tool['name']}: {tool['description']}")
        
        # æ‰§è¡Œå·¥å…·
        result = await meta.execute_tool(
            agent_name="technical",
            tool_name="calculate_indicators",
            arguments={"symbol": "AAPL"}
        )
        print(f"\nTool result: {json.dumps(result, indent=2, default=str)}")
        
        # å¦‚æœæœ‰API keyï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´å†³ç­–
        # decision = await meta.analyze_and_decide(
        #     symbol="AAPL",
        #     query="Should I buy AAPL based on technical analysis?"
        # )
        # print(f"\nDecision: {decision.action} (conviction: {decision.conviction})")
        # print(f"Reasoning: {decision.reasoning}")
    
    asyncio.run(main())
